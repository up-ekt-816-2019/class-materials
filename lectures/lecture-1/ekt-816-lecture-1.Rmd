---
title: EKT-816 Lecture 1
subtitle: Probability Review (1)
author: Jesse Naidoo
institute: University of Pretoria
fontsize: 10pt
output:
 beamer_presentation:
    template: ../../../../../templates/svm-latex-beamer.tex
    keep_tex: false
    incremental: true 
#    toc: false 
    slide_level: 2
 ioslides_presentation:
    smaller: true
make149: true
bibliography: ../../../../../library.bib
biblio-style: authordate1
header-includes: |
    \DeclareMathOperator*\plim {plim}
    \DeclareMathOperator*\cov {cov}
---

<style>
slides > slide.backdrop {
  background: white;
  border-bottom: 0px;
  box-shadow: 0 0 0;
}


slides > slide {
  font-family: 'Open Sans', Helvetica, Arial, sans-serif;
  border-bottom: 3px solid  #F66733;
  box-shadow:  0 3px 0 #522D80;

}

.title-slide hgroup h1 {
  color: #522D80;
}



h2 {

  color: #522D80;
}

slides > slide.dark {
  background: #522D80 !important;
  border-bottom: 0;
  box-shadow: 0 0 0;
}

.segue h2 {
  color: white;
}

slides > slide.title-slide {
  border-bottom: 0;
  box-shadow: 0 0 0;
}

ol, ul {

padding-bottom: 10px;

}

</style>


# Univariate Distributions

## PDFs, CDFs, and Quantiles

- Discrete distribution:
    - mass functions: $f(x) = P(X = x)$. 
    - cumulative distribution functions: $F(x) = P(X \leq x)$. 
    - Examples: Bernoulli$(p)$; binomial$(n,p)$; Poisson$(\lambda)$.
- continuous distributions:
    - density function $f_X(x)$ such that
    $$
    \int^\infty_{-\infty}\!\!f_X(x)dx = 1.
    $$
    - CDF $F_X(x)$ is increasing and such that $\lim_{x\rightarrow -\infty} F_X(x) = 0$, $\lim_{x\rightarrow \infty}F_X(x) = 1$.
    - $F'_X(x) = f_X(x)$, or
    $$
    F_X(x) = \int^x_{-\infty}\!\! f_X(t)dt
    $$
    - the $\tau$-th quantile of the distribution $F$ is the value $x_\tau$ such that
  $$
  F(x_{\tau}) = \tau
  $$

## Moments

- the _mean_ of a distribution is 
  $$
  E[X] = \int_{-\infty}^\infty\!\! xf_X(x)dx
  $$
- the _variance_ of the distribution is 
  $$
  V[X] = E[(X - \mu)^2]
  $$
  where $\mu$ is the mean of the distribution
- note, these moments may not exist!
    - but, if $V[X] < \infty$, the mean will exist (why?) 
    - also notice that $V[X] = E[X^2] - E[X]^2$
    - third (centered) moment is called _skewness_
    - fourth (centered) moment is called _kurtosis_

## Example: Pareto Distributions

- let $\alpha > 0$ be some constant
- density is 
  $$
  f_X(x) = \left\{\begin{array}{ll} \alpha x^{-(\alpha+1)} & \text{if } x > 1 \\ 0 & \text{else} \end{array}\right.
  $$
    - what is the CDF, $F_X(x)$?
    - what is the mean, $E[X]$? do we have to impose any conditions to ensure the mean exists?
    - what is the variance, $V[X]$?

## Inverse CDF Trick

- suppose we want to generate random numbers from some distribution with CDF $F$
  - we can compute $F$ and $F^{-1}$
  - we can generate uniformly distributed random numbers, $U \sim U(0,1)$
- then, you can generate $X \sim F$ as follows:
  $$
  X = F^{-1}(U)
  $$
- proof: let's find the CDF of such $X$s. 
    - let $x$ be an arbitrary number; we're going to show that $P(X \leq x) = F(x)$
    - $P(X \leq x) = P(F^{-1}(U) \leq x) = P(U \leq F(x)) = F(x)$

# Joint Distributions

## Marginal and Conditional Distributions

- take a joint density $f_{XY}(x,y)$ that integrates to 1 over $\mathbb{R}^2$
    - the marginal density of $X$ is
    $$
    f_X(x) = \int^\infty_{-\infty} \!\! f_{XY}(x,y)dy
    $$
    - analogous for marginal of $Y$
- the _conditional_ density of $Y$ _given that_ $X = x$ is
  $$
  f_{Y|X}(y|X = x) = \frac{f_{XY}(x, y)}{f_X(x)}
  $$

## Basic Rules

- expectations are linear: $E[aX + Y] = aE[X] + E[Y]$
- $V[aX] = a^2V[X]$
- $V[X+Y] = V[X] + V[Y] + 2\cov(X,Y)$

## Example: Zero Correlation, But Not Independent

- consider the following distribution:
        \begin{eqnarray}
        f_{XY}(x,y) & = & \left\{\begin{array}{ll}3/4 & \text{ if } x \in (-1,1) \text{ and } y \in (0, 1-x^2)\\ 0 & \text{otherwise} \end{array}\right.
        \end{eqnarray}
- show that $\cov(X,Y) = 0$ 
    - yet, the two are not independent!
    - to see this, compute the conditional expectation $E[Y|X]$

## Law of Iterated Expectations and Variance Decomposition

- law of iterated expectations: 
  $$
  E[E[Y|X]] = E[Y]
  $$
- variance decomposition: 
  $$
  V[Y] = V[E[Y|X]] + E[V[Y|X]]
  $$

# Classical (Frequentist) Estimation

## Classical Statistical Paradigm

- sample data $X_1, \ldots X_n$ are draws from some _data-generating process_ $f(x|\theta)$
    - $\theta$: a vector of parameters - unknown to us
    - our goal is to learn about $\theta$ from the sample
- a _statistic_ is any function of the data (or _known_ parameters)
    - as such, they are themselves random variables 
    - and, they have a distribution
    - which we would like to characterize as much as possible
- why do we care about this? want to answer two questions
    - Given enough data, will our estimate "eventually" get "close" to $\theta_0$?
    - For any fixed sample, how "close" is our estimate "likely" to be to the truth $\theta_0$?
- asymptotic theory is useful because it allows us to answer these questions 
    - using a precise meaning for the words "close", "eventually", and "likely"


## Modes of Convergence

- $(X_n)_{n=1}^\infty$ converges _in probability_ to the random variable $X$ if for all $\varepsilon > 0$,
        \begin{eqnarray}
        \lim_{n\rightarrow\infty}P(|X_n - X| > \varepsilon) & = & 0
        \end{eqnarray}
    - We write $X_n \longrightarrow_p X$ as shorthand. 
- $(X_n)_{n=1}^\infty$ converges \emph{in mean square} to the random variable $X$ if
        \begin{eqnarray}
        \lim_{n\rightarrow\infty}E[(X_n - X)^2] & = & 0
        \end{eqnarray}
    - We write $X_n \longrightarrow_{m.s.} X$ as shorthand.
- Convergence in mean square implies convergence in probability, but _not_ vice versa

## Law(s) of Large Numbers

- consider $(X_i)_{i=1}^{\infty}$ i.i.d. with mean $\mu$ and variance $\sigma^2$
    - let $\overline{X}_n = n^{-1}\sum^n_{k=1}X_k$
    - $V[\overline{X}_n] = n^{-1}\sigma^2$, so $\overline{X}_n \longrightarrow_{m.s.} 0$
    - thus, $\overline{X}_n \longrightarrow_p 0$ also
- we say "the sample mean is _consistent_ for the population mean"
- consistency is a necessary condition for an estimator to be useful
    - if you're never going to get the truth out of this calculation, why bother?

## Central Limit Theorems

- besides consistency we would like to know about the _precision_ of our estimates
    - it is good to know that we get to the truth "eventually", but how close are we right now?
    - we need a different notion of convergence to characterize the asymptotic approximation here
- _convergence in distribution_: we say $(X_n)_{n=1}^\infty \longrightarrow_d X$ if,
    - for every point $a$ where $F_X(\cdot)$ is continuous, $P(X_n \leq a) \longrightarrow P(X \leq a)$
- _Central Limit Theorem_: 
    - if $(X_i)_{i=1}^\infty$ is an i.i.d. sample from a distribution with $V[X] = \sigma^2 < \infty$ and $E[X]  = \mu$, then 
        \begin{eqnarray}
        \sqrt{n}\left(\frac{n^{-1}\sum^n_{k=1}X_k - \mu}{\sigma}\right) & \longrightarrow_d & N(0,1)
        \end{eqnarray}
    - this suggests we may use the approximation
        \begin{eqnarray}
        P(\overline{X}_n \leq a) & \approx & \Phi\left(\frac{a - \mu}{\sigma/\sqrt{n}}\right)
        \end{eqnarray}

## Monte Carlo Simulations

- we need a way to examine the sampling distribution of some estimator $\widehat{\theta}(X_1, \ldots X_n)$
  - except for special cases (e.g. the mean of normal observations), we will not be able to calculate the distribution of a general function of the data for arbitrary sample sizes
- solution: approximate the distribution of $\widehat{\theta}(X_1, \ldots X_n)$ by simulating some large number of datasets (each of size $n$)
- pseudocode:
  - $B$ is number of simulated datasets
  - for each $b = 1, \ldots B$ we generate a dataset of size $n$; compute $\widehat{\theta}_{n,b}$
  - treat the resulting sample $\widehat{\theta}_{n,1}, \widehat{\theta}_{n,2}, \ldots \widehat{\theta}_{n,B}$ as the population distribution

## Monte Carlo Simulations

```
for n = 10, 100, 1000, 10000 {
   for b = 1, ... B {
```
\par\hspace{10mm}\texttt{draw } $X_1 \ldots X_n$ \texttt{ from } $f(X|\theta_0)$\texttt{;}
\par\hspace{10mm}\texttt{calculate } $\widehat{\theta}_{n,b}$ \texttt{ from } $X_1 \ldots X_n$\texttt{;}
\par\hspace{10mm}\texttt{store } $\widehat{\theta}_{n,b}$\texttt{;}
\par\hspace{10mm}\texttt{discard } $X_1 \ldots X_n$\texttt{;} 
```
   }
}
```

- then, using $\widehat{\theta}_{n,1}, \widehat{\theta}_{n,2}, \ldots \widehat{\theta}_{n,B}$:
  - plot the density of $(\widehat{\theta}_{n,b})_{b=1}^B$ for $n = 10, 100, 1000, 10000$
  - compute the variance of $(\widehat{\theta}_{n,b})_{b=1}^B$ for $n = 10, 100, 1000, 10000$
  - etc.

## References
